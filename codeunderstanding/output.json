{
 "question": "Explain the python code given before.   Describe all the classes and relative methods.     Explain in detail the first 3 function of the class Env.",
 "answer": [
  "The given code defines two classes: `Env` and `RLEnv`.\n\nThe `Env` class is the fundamental environment class for the Habitat simulator. It serves as the base class for other derived environment classes. It consists of three major components: a dataset (episodes), a simulator, and a task. It connects these components together to create a simulation environment.\n\nThe `Env` class has the following attributes:\n- `observation_space`: A `SpaceDict` object corresponding to the sensor in the simulator and the task.\n- `action_space`: A `gym.space` object corresponding to the valid actions.\n- `_config`: A dictionary containing the configuration for the environment.\n- `_dataset`: An optional dataset for the environment.\n- `number_of_episodes`: The number of episodes in the dataset.\n- `_current_episode`: The current episode being used in the environment.\n- `_episode_iterator`: An iterator over the episodes in the dataset.\n- `_sim`: The simulator object.\n- `_task`: The task object.\n- `_max_episode_seconds`: The maximum allowed duration of an episode in seconds.\n- `_max_episode_steps`: The maximum allowed number of steps in an episode.\n- `_elapsed_steps`: The number of steps taken in the current episode.\n- `_episode_start_time`: The start time of the current episode.\n- `_episode_over`: A boolean indicating if the current episode is over.\n- `_episode_from_iter_on_reset`: A boolean indicating if the episode iterator should be used on reset.\n- `_episode_force_changed`: A boolean indicating if the episode has been changed.\n\nThe `Env` class has the following methods:\n- `__init__(self, config: \"DictConfig\", dataset: Optional[Dataset[Episode]] = None)`: The constructor method for the `Env` class. It initializes the environment with the given configuration and dataset.\n- `_setup_episode_iterator(self)`: Sets up the episode iterator for the dataset.\n- `reset(self) -> Observations`: Resets the environment and returns the initial observations.\n- `_update_step_stats(self)`: Updates the step statistics for the environment.\n- `step(self, action: Union[int, str, Dict[str, Any]], **kwargs) -> Observations`: Performs an action in the environment and returns the observations.\n- `seed(self, seed: int) -> None`: Seeds the environment with the given seed.\n- `reconfigure(self, config: \"DictConfig\") -> None`: Reconfigures the environment with the given configuration.\n- `render(self, mode=\"rgb\") -> np.ndarray`: Renders the environment and returns the rendered image.\n- `close(self) -> None`: Closes the environment.\n- `get_metrics(self) -> Metrics`: Returns the metrics for the current episode.\n- `__enter__(self)`: Method used as a context manager.\n- `__exit__(self, exc_type, exc_val, exc_tb)`: Method called when exiting the context manager.\n\nThe `RLEnv` class is a subclass of `gym.Env` and serves as a wrapper over the `Env` class for reinforcement learning (RL) applications. It provides a simple interface for RL agents to interact with the Habitat simulator.\n\nThe `RLEnv` class has the following attributes:\n- `_env`: An instance of the `Env` class.\n\nThe `RLEnv` class has the following methods:\n- `__init__(self, config: \"DictConfig\", dataset: Optional[Dataset] = None)`: The constructor method for the `RLEnv` class. It initializes the RL environment with the given configuration and dataset.\n- `reset(self, *, return_info: bool = False, **kwargs) -> Union[Observations, Tuple[Observations, Dict]]`: Resets the RL environment and returns the initial observations.\n- `get_reward_range(self)`: Returns the range of possible reward values.\n- `get_reward(self, observations: Observations) -> Any`: Returns the reward after performing the last action.\n- `get_done(self, observations: Observations) -> bool`: Returns a boolean indicating if the episode is done after performing the last action.\n- `get_info(self, observations) -> Dict[Any, Any]`: Returns additional information after performing the last action.\n- `step(self, *args, **kwargs) -> Tuple[Observations, Any, bool, dict]`: Performs an action in the RL environment and returns the observations, reward, done flag, and info dictionary.\n- `seed(self, seed: Optional[int] = None) -> None`: Seeds the RL environment with the given seed.\n- `render(self, mode: str = \"rgb\") -> np.ndarray`: Renders the RL environment and returns the rendered image.\n- `close(self) -> None`: Closes the RL environment.\n- `__enter__(self)`: Method used as a context manager.\n- `__exit__(self, exc_type, exc_val, exc_tb)`: Method called when exiting the context manager.\n\nNow, let's discuss the first three methods of the `Env` class in detail:\n\n1. `__init__(self, config: \"DictConfig\", dataset: Optional[Dataset[Episode]] = None) -> None`: This is the constructor method of the `Env` class. It takes a configuration dictionary and an optional dataset as input. It initializes the environment with the given configuration and dataset. It sets up the episode iterator if a dataset is provided. It also initializes the simulator and task objects based on the configuration. It sets the observation space and action space based on the sensor suites of the simulator and task. It sets the maximum episode duration and maximum episode steps from the configuration. It also initializes some internal variables related to the episode status.\n\n2. `_setup_episode_iterator(self)`: This method sets up the episode iterator for the dataset. It creates an iterator over the episodes in the dataset based on the iterator options specified in the environment configuration. It also sets the seed for the episode iterator.\n\n3. `reset(self) -> Observations`: This method resets the environment and returns the initial observations. It resets the episode statistics, deletes the shortest path cache of the current episode, and selects a new episode if the episode iterator is set. It reconfigures the environment based on the current episode and resets the task's measurements. Finally, it returns the observations after resetting the task.\n\nThese methods are used to initialize and reset the environment, setting up the necessary components and preparing the environment for interaction.",
  "The given code defines three classes: `Env`, `RLEnv`, and their parent class `gym.Env`.\n\n1. `Env` class:\n   - This class is the fundamental environment class for the `habitat` library, which is used for embodied tasks.\n   - It consists of three major components: `dataset` (episodes), `simulator`, and `task`. It connects these components together.\n   - The constructor takes a configuration and an optional dataset. It initializes the environment by loading the first scene if a dataset is provided and sets up the simulator and task.\n   - The `observation_space` attribute represents the observation space of the environment, and the `action_space` attribute represents the action space.\n   - The class provides methods for resetting the environment, taking a step, getting metrics, seeding the environment, rendering, and closing the simulator.\n\n2. `RLEnv` class:\n   - This class is a wrapper over the `Env` class for reinforcement learning (RL) environments.\n   - It subclasses `gym.Env` and defines methods specific to RL environments: `get_reward_range()`, `get_reward()`, `get_done()`, and `get_info()`.\n   - The constructor takes a configuration and an optional dataset. It initializes the `Env` and sets the observation space, action space, number of episodes, and reward range.\n   - It provides methods for resetting the environment, taking a step, seeding, rendering, and closing.\n\n3. `gym.Env` class:\n   - This is the parent class for the `RLEnv` class, which subclasses it.\n   - It is a generic environment class provided by the OpenAI Gym library that defines the basic interface for RL environments.\n   - It defines methods like `reset()`, `step()`, `render()`, `close()`, etc., which are overridden in the `RLEnv` class to provide the specific functionality for the `habitat` environment.\n\nNow, let's discuss the first three methods of the `Env` class in detail:\n\n- `__init__(self, config: DictConfig, dataset: Optional[Dataset[Episode]] = None) -> None`:\n  - This method is the constructor of the `Env` class.\n  - It takes a configuration object (`config`) and an optional dataset object (`dataset`) as input.\n  - Inside the constructor, the configuration is processed, and the dataset is loaded if provided.\n  - The simulator and task are created using the configuration and dataset, and the observation and action spaces are set accordingly.\n  - The maximum episode duration and maximum number of steps are initialized, and other variables related to episode tracking are set.\n  - Finally, the constructor sets the episode iterator and loads the first episode if a dataset is provided.\n\n- `_setup_episode_iterator(self)`:\n  - This is a private method that sets up the episode iterator using the current dataset.\n  - It converts the iterator options from the configuration to a dictionary and passes them to the `get_episode_iterator()` method of the dataset.\n  - The seed is also set in the options to ensure reproducibility.\n  - This method is called during initialization if a dataset is provided.\n\n- `reset(self) -> Observations`:\n  - This method resets the environment and returns the initial observations.\n  - It resets various tracking variables, including the episode start time, elapsed steps, and episode over flag.\n  - It clears the shortest path cache of the current episode and gets the next episode from the iterator if it was set in the constructor.\n  - The environment configuration is updated based on the current episode.\n  - The `reset()` method of the task is called, passing the current episode, and the observations are returned.\n  - The environment is now ready for a new episode.",
  "The given Python code defines three classes: `Env`, `RLEnv`, and `Observations`. \n\n1. `Env`:\n   - This class is the fundamental environment class for the Habitat framework. It acts as a base for other derived environment classes.\n   - It contains three major components: `dataset` (episodes), `simulator`, and `task`, which are connected together to create an environment for an embodied task.\n   - The constructor takes a configuration and an optional dataset as input. It initializes the environment by creating a dataset, setting up the episode iterator, creating a simulator, and creating the task.\n   - The class provides methods for resetting the environment, taking a step in the environment, getting metrics, seeding the environment, rendering, closing the environment, and more.\n   - The `reset` method resets the environment and returns the initial observations.\n   - The `step` method performs an action in the environment and returns the observations after taking the action.\n   - The `reconfigure` method updates the configuration of the environment.\n   - The `render` method renders the current state of the environment.\n   - The `close` method closes the environment.\n\n2. `RLEnv`:\n   - This class is a wrapper over `Env` class and subclasses the `gym.Env` class, making it suitable for reinforcement learning (RL) tasks.\n   - It provides methods for getting the reward range, calculating the reward, checking if the episode is done, and getting info about the current state.\n   - The `reset` method resets the environment and returns the initial observations.\n   - The `step` method performs an action in the environment and returns the observations, reward, done flag, and info.\n   - It also provides methods for seeding the environment, rendering, and closing the environment.\n\n3. `Observations`:\n   - This class represents the observations from the simulator and the task in the environment.\n   - It is a dictionary-like object that contains the sensor observations as key-value pairs.\n   - It provides methods for accessing and updating the observations based on the sensor names.\n\nNow, let's discuss the first three methods of the `Env` class in detail:\n\n1. `__init__(self, config: \"DictConfig\", dataset: Optional[Dataset[Episode]] = None) -> None`:\n   - This is the constructor of the `Env` class.\n   - It takes a configuration and an optional dataset as input.\n   - Inside the constructor, it initializes the environment by creating a dataset, setting up the episode iterator, creating a simulator, and creating the task.\n   - It also sets the observation space and action space based on the sensor suites of the simulator and task.\n   - It initializes other attributes such as the maximum episode seconds, maximum episode steps, elapsed steps, episode start time, and episode over flag.\n\n2. `_setup_episode_iterator(self)`:\n   - This method is used to set up the episode iterator.\n   - It is called internally when the environment has a dataset and needs to iterate over the episodes.\n   - It creates a dictionary of iterator options by parsing the environment configuration.\n   - It then calls the `get_episode_iterator` method of the dataset, passing the iterator options as arguments, to obtain the episode iterator.\n\n3. `reset(self) -> Observations`:\n   - This method resets the environment and returns the initial observations.\n   - It is called at the beginning of each episode.\n   - It resets the elapsed steps, episode over flag, and episode start time.\n   - If the episode iterator is set and the episode was changed on the previous reset, it gets the next episode from the iterator and updates the simulator and task configurations accordingly.\n   - It then calls the `reset` method of the task, passing the current episode, and obtains the initial observations from the task.\n   - Finally, it resets the measurements of the task and returns the observations.",
  "The given code defines three classes: `Env`, `RLEnv`, and `BaseEpisode`.\n\n1. `Env` is the fundamental environment class for the Habitat framework. It acts as a base for other derived environment classes and consists of three major components: a dataset, a simulator, and an embodied task. The `Env` class has several properties and methods:\n\n   - `observation_space`: A `spaces.Dict` object representing the observation spaces of the simulator and the task.\n   - `action_space`: A `spaces.Dict` object representing the valid action spaces.\n   - `_config`: A configuration object for the environment.\n   - `_dataset`: The dataset used for the environment.\n   - `number_of_episodes`: The number of episodes in the dataset.\n   - `_current_episode`: The current episode being used.\n   - `_episode_iterator`: An iterator over the episodes in the dataset.\n   - `_sim`: The simulator used in the environment.\n   - `_task`: The embodied task used in the environment.\n   - `_max_episode_seconds`: The maximum duration of an episode in seconds.\n   - `_max_episode_steps`: The maximum number of steps allowed in an episode.\n   - `_elapsed_steps`: The number of steps taken in the current episode.\n   - `_episode_start_time`: The start time of the current episode.\n   - `_episode_over`: A boolean flag indicating if the current episode is over.\n   - `_episode_from_iter_on_reset`: A boolean flag indicating if the next episode is taken from the iterator on reset.\n   - `_episode_force_changed`: A boolean flag indicating if the episode was changed either by setting `current_episode` or changing the episodes list.\n   \n   The `Env` class has methods such as `__init__`, `_setup_episode_iterator`, `current_episode`, `current_episode.setter`, `episode_iterator`, `episode_iterator.setter`, `episodes`, `episodes.setter`, `sim`, `episode_start_time`, `episode_over`, `task`, `get_metrics`, `_past_limit`, `_reset_stats`, `reset`, `_update_step_stats`, `step`, `seed`, `reconfigure`, `render`, `close`, `__enter__`, and `__exit__`.\n\n2. `RLEnv` is a reinforcement learning (RL) environment class that subclasses `gym.Env`. It is a wrapper over the `Env` class for RL users. It provides methods for RL-specific functionality such as getting the reward range, computing the reward, checking if the episode is done, and getting additional information. The `RLEnv` class has properties and methods similar to the `Env` class, but also includes methods like `get_reward_range`, `get_reward`, `get_done`, `get_info`, `__init__`, `config`, `habitat_env`, `current_episode`, `reset`, and `step`.\n\n3. `BaseEpisode` is a simple data class that represents an episode in the dataset. It has two properties: `episode_id` and `scene_id`, which store the IDs of the episode and the scene, respectively. This class is used in the `RLEnv` class to provide information about the current episode.\n\nNow, let's discuss the first three methods of the `Env` class in detail:\n\n- `__init__(self, config: \"DictConfig\", dataset: Optional[Dataset[Episode]] = None) -> None`: This is the constructor method of the `Env` class. It initializes the environment by setting the configuration, dataset, current episode, episode iterator, simulator, and task. It also sets the observation space and action space based on the simulator and task. Additionally, it sets the maximum episode duration and maximum number of steps allowed in an episode.\n\n- `_setup_episode_iterator(self)`: This method sets up the episode iterator using the dataset and the iterator options specified in the environment configuration. It converts the iterator options to lowercase and adds the seed value from the environment configuration. This method is called internally to initialize the episode iterator when the dataset is present.\n\n- `reset(self) -> Observations`: This method resets the environment and returns the initial observations. It first resets the statistics and caches of the current episode. If an episode iterator is present and the episode was changed on the previous reset, it fetches the next episode from the iterator. It then calls the `reset` method of the task to reset the task-specific state and measurements. Finally, it returns the initial observations from the task.\n\nThese methods provide the basic functionality for initializing and resetting the environment in the Habitat framework.",
  "The given code defines three classes: `Env`, `RLEnv`, and `Episode`. \n\nThe `Env` class is the fundamental environment class for the Habitat framework. It acts as a base for other derived environment classes. It consists of three major components: `dataset`, `simulator`, and `task`. It connects all three components together. The `Env` class provides methods for resetting the environment, taking actions, and getting observations, as well as properties for accessing the current episode and other environment variables.\n\nThe `Env` class has the following important methods:\n- `__init__(self, config: DictConfig, dataset: Optional[Dataset[Episode]] = None)`: This is the constructor method that initializes the `Env` object. It takes a configuration dictionary and an optional dataset object as input. It sets up the dataset, simulator, and task based on the provided configuration.\n- `_setup_episode_iterator(self)`: This method is called to set up the episode iterator for the environment. It creates an iterator object based on the dataset and configuration.\n- `reset(self) -> Observations`: This method resets the environment and returns the initial observations. It sets up a new episode if the episode iterator is provided. It also resets the task measurements and starts tracking the elapsed time and steps.\n- `step(self, action: Union[int, str, Dict[str, Any]], **kwargs) -> Observations`: This method performs an action in the environment and returns the resulting observations. It updates the task measurements, checks if the episode is over, and updates the step statistics.\n- `render(self, mode=\"rgb\") -> np.ndarray`: This method renders the current state of the environment and returns it as an image.\n- `close(self) -> None`: This method closes the environment and frees any resources used by the simulator.\n\nThe `RLEnv` class is a subclass of `gym.Env` and serves as a wrapper over the `Env` class for reinforcement learning tasks. It provides methods for getting the reward, done flag, and info after performing an action in the environment. It also implements the `reset()` and `step()` methods required by the `gym.Env` interface.\n\nThe `Episode` class represents an episode in the environment. It stores information such as the episode ID, scene ID, and scene dataset configuration. It also provides methods for accessing and modifying the episode's shortest path cache.\n\nIn summary, the `Env` class provides the core functionality for the habitat environment, while the `RLEnv` class extends it for reinforcement learning tasks. The `Episode` class represents individual episodes in the environment."
 ],
 "context": "\n\nimport random\nimport time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nimport gym\nimport numba\nimport numpy as np\nfrom gym import spaces\n\nfrom habitat.config import read_write\nfrom habitat.core.dataset import BaseEpisode, Dataset, Episode, EpisodeIterator\nfrom habitat.core.embodied_task import EmbodiedTask, Metrics\nfrom habitat.core.simulator import Observations, Simulator\nfrom habitat.datasets import make_dataset\nfrom habitat.sims import make_sim\nfrom habitat.tasks.registration import make_task\nfrom habitat.utils import profiling_wrapper\n\nif TYPE_CHECKING:\n    from omegaconf import DictConfig\n\n\nclass Env:\n    r\"\"\"Fundamental environment class for :ref:`habitat`.\n\n    :data observation_space: ``SpaceDict`` object corresponding to sensor in\n        sim and task.\n    :data action_space: ``gym.space`` object corresponding to valid actions.\n\n    All the information  needed for working on embodied task with simulator\n    is abstracted inside :ref:`Env`. Acts as a base for other derived\n    environment classes. :ref:`Env` consists of three major components:\n    ``dataset`` (`episodes`), ``simulator`` (:ref:`sim`) and :ref:`task` and\n    connects all the three components together.\n    \"\"\"\n\n    observation_space: spaces.Dict\n    action_space: spaces.Dict\n    _config: \"DictConfig\"\n    _dataset: Optional[Dataset[Episode]]\n    number_of_episodes: Optional[int]\n    _current_episode: Optional[Episode]\n    _episode_iterator: Optional[Iterator[Episode]]\n    _sim: Simulator\n    _task: EmbodiedTask\n    _max_episode_seconds: int\n    _max_episode_steps: int\n    _elapsed_steps: int\n    _episode_start_time: Optional[float]\n    _episode_over: bool\n    _episode_from_iter_on_reset: bool\n    _episode_force_changed: bool\n\n    def __init__(\n        self, config: \"DictConfig\", dataset: Optional[Dataset[Episode]] = None\n    ) -> None:\n        \"\"\"Constructor\n\n        :param config: config for the environment. Should contain id for\n            simulator and ``task_name`` which are passed into ``make_sim`` and\n            ``make_task``.\n        :param dataset: reference to dataset for task instance level\n            information. Can be defined as :py:`None` in which case\n            ``_episodes`` should be populated from outside.\n        \"\"\"\n\n        if \"habitat\" in config:\n            config = config.habitat\n        self._config = config\n        self._dataset = dataset\n        if self._dataset is None and config.dataset.type:\n            self._dataset = make_dataset(\n                id_dataset=config.dataset.type, config=config.dataset\n            )\n\n        self._current_episode = None\n        self._episode_iterator = None\n        self._episode_from_iter_on_reset = True\n        self._episode_force_changed = False\n\n        # load the first scene if dataset is present\n        if self._dataset:\n            assert (\n                len(self._dataset.episodes) > 0\n            ), \"dataset should have non-empty episodes list\"\n            self._setup_episode_iterator()\n            self.current_episode = next(self.episode_iterator)\n            with read_write(self._config):\n                self._config.simulator.scene_dataset = (\n                    self.current_episode.scene_dataset_config\n                )\n                self._config.simulator.scene = self.current_episode.scene_id\n\n            self.number_of_episodes = len(self.episodes)\n        else:\n            self.number_of_episodes = None\n\n        self._sim = make_sim(\n            id_sim=self._config.simulator.type, config=self._config.simulator\n        )\n\n        self._task = make_task(\n            self._config.task.type,\n            config=self._config.task,\n            sim=self._sim,\n            dataset=self._dataset,\n        )\n        self.observation_space = spaces.Dict(\n            {\n                **self._sim.sensor_suite.observation_spaces.spaces,\n                **self._task.sensor_suite.observation_spaces.spaces,\n            }\n        )\n        self.action_space = self._task.action_space\n        self._max_episode_seconds = (\n            self._config.environment.max_episode_seconds\n        )\n        self._max_episode_steps = self._config.environment.max_episode_steps\n        self._elapsed_steps = 0\n        self._episode_start_time: Optional[float] = None\n        self._episode_over = False\n\n    def _setup_episode_iterator(self):\n        assert self._dataset is not None\n        iter_option_dict = {\n            k.lower(): v\n            for k, v in self._config.environment.iterator_options.items()\n        }\n        iter_option_dict[\"seed\"] = self._config.seed\n        self._episode_iterator = self._dataset.get_episode_iterator(\n            **iter_option_dict\n        )\n\n    @property\n    def current_episode(self) -> Episode:\n        assert self._current_episode is not None\n        return self._current_episode\n\n    @current_episode.setter\n    def current_episode(self, episode: Episode) -> None:\n        self._current_episode = episode\n        # This allows the current episode to be set here\n        # and then reset be called without the episode changing\n        self._episode_from_iter_on_reset = False\n        self._episode_force_changed = True\n\n    @property\n    def episode_iterator(self) -> Iterator[Episode]:\n        return self._episode_iterator\n\n    @episode_iterator.setter\n    def episode_iterator(self, new_iter: Iterator[Episode]) -> None:\n        self._episode_iterator = new_iter\n        self._episode_force_changed = True\n        self._episode_from_iter_on_reset = True\n\n    @property\n    def episodes(self) -> List[Episode]:\n        return (\n            self._dataset.episodes\n            if self._dataset\n            else cast(List[Episode], [])\n        )\n\n    @episodes.setter\n    def episodes(self, episodes: List[Episode]) -> None:\n        assert (\n            len(episodes) > 0\n        ), \"Environment doesn't accept empty episodes list.\"\n        assert (\n            self._dataset is not None\n        ), \"Environment must have a dataset to set episodes\"\n        self._dataset.episodes = episodes\n        self._setup_episode_iterator()\n        self._current_episode = None\n        self._episode_force_changed = True\n        self._episode_from_iter_on_reset = True\n\n    @property\n    def sim(self) -> Simulator:\n        return self._sim\n\n    @property\n    def episode_start_time(self) -> Optional[float]:\n        return self._episode_start_time\n\n    @property\n    def episode_over(self) -> bool:\n        return self._episode_over\n\n    @property\n    def task(self) -> EmbodiedTask:\n        return self._task\n\n    @property\n    def _elapsed_seconds(self) -> float:\n        assert (\n            self._episode_start_time\n        ), \"Elapsed seconds requested before episode was started.\"\n        return time.time() - self._episode_start_time\n\n    def get_metrics(self) -> Metrics:\n        return self._task.measurements.get_metrics()\n\n    def _past_limit(self) -> bool:\n        return (\n            self._max_episode_steps != 0\n            and self._max_episode_steps <= self._elapsed_steps\n        ) or (\n            self._max_episode_seconds != 0\n            and self._max_episode_seconds <= self._elapsed_seconds\n        )\n\n    def _reset_stats(self) -> None:\n        self._episode_start_time = time.time()\n        self._elapsed_steps = 0\n        self._episode_over = False\n\n    def reset(self) -> Observations:\n        r\"\"\"Resets the environments and returns the initial observations.\n\n        :return: initial observations from the environment.\n        \"\"\"\n        self._reset_stats()\n\n        # Delete the shortest path cache of the current episode\n        # Caching it for the next time we see this episode isn't really worth\n        # it\n        if self._current_episode is not None:\n            self._current_episode._shortest_path_cache = None\n\n        if (\n            self._episode_iterator is not None\n            and self._episode_from_iter_on_reset\n        ):\n            self._current_episode = next(self._episode_iterator)\n\n        # This is always set to true after a reset that way\n        # on the next reset an new episode is taken (if possible)\n        self._episode_from_iter_on_reset = True\n        self._episode_force_changed = False\n\n        assert self._current_episode is not None, \"Reset requires an episode\"\n        self.reconfigure(self._config)\n\n        observations = self.task.reset(episode=self.current_episode)\n        self._task.measurements.reset_measures(\n            episode=self.current_episode,\n            task=self.task,\n            observations=observations,\n        )\n\n        return observations\n\n    def _update_step_stats(self) -> None:\n        self._elapsed_steps += 1\n        self._episode_over = not self._task.is_episode_active\n        if self._past_limit():\n            self._episode_over = True\n\n        if self.episode_iterator is not None and isinstance(\n            self.episode_iterator, EpisodeIterator\n        ):\n            self.episode_iterator.step_taken()\n\n    def step(\n        self, action: Union[int, str, Dict[str, Any]], **kwargs\n    ) -> Observations:\n        r\"\"\"Perform an action in the environment and return observations.\n\n        :param action: action (belonging to :ref:`action_space`) to be\n            performed inside the environment. Action is a name or index of\n            allowed task's action and action arguments (belonging to action's\n            :ref:`action_space`) to support parametrized and continuous\n            actions.\n        :return: observations after taking action in environment.\n        \"\"\"\n\n        assert (\n            self._episode_start_time is not None\n        ), \"Cannot call step before calling reset\"\n        assert (\n            self._episode_over is False\n        ), \"Episode over, call reset before calling step\"\n        assert (\n            not self._episode_force_changed\n        ), \"Episode was changed either by setting current_episode or changing the episodes list. Call reset before stepping the environment again.\"\n\n        # Support simpler interface as well\n        if isinstance(action, (str, int, np.integer)):\n            action = {\"action\": action}\n\n        observations = self.task.step(\n            action=action, episode=self.current_episode\n        )\n\n        self._task.measurements.update_measures(\n            episode=self.current_episode,\n            action=action,\n            task=self.task,\n            observations=observations,\n        )\n\n        self._update_step_stats()\n\n        return observations\n\n    @staticmethod\n    @numba.njit\n    def _seed_numba(seed: int):\n        random.seed(seed)\n        np.random.seed(seed)\n\n    def seed(self, seed: int) -> None:\n        random.seed(seed)\n        np.random.seed(seed)\n        self._seed_numba(seed)\n        self._sim.seed(seed)\n        self._task.seed(seed)\n\n    def reconfigure(self, config: \"DictConfig\") -> None:\n        self._config = self._task.overwrite_sim_config(\n            config, self.current_episode\n        )\n\n        self._sim.reconfigure(self._config.simulator, self.current_episode)\n\n    def render(self, mode=\"rgb\") -> np.ndarray:\n        return self._sim.render(mode)\n\n    def close(self) -> None:\n        self._sim.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\nclass RLEnv(gym.Env):\n    r\"\"\"Reinforcement Learning (RL) environment class which subclasses ``gym.Env``.\n\n    This is a wrapper over :ref:`Env` for RL users. To create custom RL\n    environments users should subclass `RLEnv` and define the following\n    methods: :ref:`get_reward_range()`, :ref:`get_reward()`,\n    :ref:`get_done()`, :ref:`get_info()`.\n\n    As this is a subclass of ``gym.Env``, it implements `reset()` and\n    `step()`.\n    \"\"\"\n\n    _env: Env\n\n    def __init__(\n        self, config: \"DictConfig\", dataset: Optional[Dataset] = None\n    ) -> None:\n        \"\"\"Constructor\n\n        :param config: config to construct :ref:`Env`\n        :param dataset: dataset to construct :ref:`Env`.\n        \"\"\"\n        if \"habitat\" in config:\n            config = config.habitat\n        self._core_env_config = config\n        self._env = Env(config, dataset)\n        self.observation_space = self._env.observation_space\n        self.action_space = self._env.action_space\n        self.number_of_episodes = self._env.number_of_episodes\n        self.reward_range = self.get_reward_range()\n\n    @property\n    def config(self) -> \"DictConfig\":\n        return self._core_env_config\n\n    @property\n    def habitat_env(self) -> Env:\n        return self._env\n\n    @property\n    def episodes(self) -> List[Episode]:\n        return self._env.episodes\n\n    @episodes.setter\n    def episodes(self, episodes: List[Episode]) -> None:\n        self._env.episodes = episodes\n\n    def current_episode(self, all_info: bool = False) -> BaseEpisode:\n        r\"\"\"Returns the current episode of the environment.\n\n        :param all_info: If true, all the information in the episode\n                         will be provided. Otherwise, only episode_id\n                         and scene_id will be included.\n        :return: The BaseEpisode object for the current episode.\n        \"\"\"\n        if all_info:\n            return self._env.current_episode\n        else:\n            return BaseEpisode(\n                episode_id=self._env.current_episode.episode_id,\n                scene_id=self._env.current_episode.scene_id,\n            )\n\n    @profiling_wrapper.RangeContext(\"RLEnv.reset\")\n    def reset(\n        self, *, return_info: bool = False, **kwargs\n    ) -> Union[Observations, Tuple[Observations, Dict]]:\n        observations = self._env.reset()\n        if return_info:\n            return observations, self.get_info(observations)\n        else:\n            return observations\n\n    def get_reward_range(self):\n        r\"\"\"Get min, max range of reward.\n\n        :return: :py:`[min, max]` range of reward.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_reward(self, observations: Observations) -> Any:\n        r\"\"\"Returns reward after action has been performed.\n\n        :param observations: observations from simulator and task.\n        :return: reward after performing the last action.\n\n        This method is called inside the :ref:`step()` method.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_done(self, observations: Observations) -> bool:\n        r\"\"\"Returns boolean indicating whether episode is done after performing\n        the last action.\n\n        :param observations: observations from simulator and task.\n        :return: done boolean after performing the last action.\n\n        This method is called inside the step method.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_info(self, observations) -> Dict[Any, Any]:\n        r\"\"\"..\n\n        :param observations: observations from simulator and task.\n        :return: info after performing the last action.\n        \"\"\"\n        raise NotImplementedError\n\n    @profiling_wrapper.RangeContext(\"RLEnv.step\")\n    def step(self, *args, **kwargs) -> Tuple[Observations, Any, bool, dict]:\n        r\"\"\"Perform an action in the environment.\n\n        :return: :py:`(observations, reward, done, info)`\n        \"\"\"\n\n        observations = self._env.step(*args, **kwargs)\n        reward = self.get_reward(observations)\n        done = self.get_done(observations)\n        info = self.get_info(observations)\n\n        return observations, reward, done, info\n\n    def seed(self, seed: Optional[int] = None) -> None:\n        self._env.seed(seed)\n\n    def render(self, mode: str = \"rgb\") -> np.ndarray:\n        return self._env.render(mode)\n\n    def close(self) -> None:\n        self._env.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n"
}